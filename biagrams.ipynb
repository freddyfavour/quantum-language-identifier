{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWd2c0Xh2_T3"
   },
   "source": [
    "# Quantum-Enhanced Bigram Language Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuBwGOSGjWQK",
    "outputId": "03ba789c-2287-4a23-de08-b059bca90c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAJY7G38Eig7",
    "outputId": "1499a6ce-c29a-4c5a-d71e-f8aac8a94963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hAll packages installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q qiskit qiskit-aer scipy pandas numpy faker\n",
    "print(\"All packages installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "opnrtKfpkZee"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "\n",
    "# Required for the Novel Training Approach\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.quantum_info import Statevector\n",
    "import scipy.optimize as optimize\n",
    "from faker import Faker\n",
    "\n",
    "# Required for the Evaluation Approach\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report , accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNpKIiD9r2d9"
   },
   "source": [
    "Load Dataset and Build Language Models\n",
    "\n",
    "Loading the bigram CSV and creating both word-bigram and character-bigram models. Character bigrams are crucial for discriminating languages like Twi that have unique orthographic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mYi27bl7Rynj"
   },
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/bigrams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJcmqwVfWAmV"
   },
   "source": [
    "# Feature Extraction Using Word and Character Bigrams\n",
    "This code builds word-level and character-level bigram language models for Twi, English, and French.\n",
    "It uses smoothed log-probabilities to score how likely a sentence belongs to each language.\n",
    "Character bigrams help handle short sentences and unseen words.\n",
    "Each sentence is converted into a compact numerical feature vector.\n",
    "These features are later used for fast and accurate language identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sPsv5JDySTo7"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, alpha: float = 0.1):\n",
    "        self.alpha = alpha\n",
    "        self.languages = [\"twi\", \"eng\", \"fra\"]\n",
    "        self.word_models = {}\n",
    "        self.char_models = {}\n",
    "\n",
    "    def build_models(self, df: pd.DataFrame):\n",
    "        logger.info(\"Building word & character bigram models...\")\n",
    "\n",
    "        for lang_id, lang in enumerate(self.languages, start=1):\n",
    "            lang_df = df[df[\"lang_id\"] == lang_id]\n",
    "\n",
    "            word_counts = dict(zip(lang_df[\"ngram\"], lang_df[\"count\"]))\n",
    "            total_words = sum(word_counts.values())\n",
    "            vocab_words = len(word_counts)\n",
    "            self.word_models[lang] = (word_counts, total_words, vocab_words)\n",
    "\n",
    "            char_counts = defaultdict(int)\n",
    "            for bg, c in word_counts.items():\n",
    "                for w in bg.split():\n",
    "                    w = \"^\" + w.lower() + \"$\"\n",
    "                    for i in range(len(w) - 1):\n",
    "                        char_counts[w[i:i+2]] += c\n",
    "\n",
    "            self.char_models[lang] = (\n",
    "                dict(char_counts),\n",
    "                sum(char_counts.values()),\n",
    "                len(char_counts)\n",
    "            )\n",
    "\n",
    "        logger.info(\"Models built successfully.\")\n",
    "\n",
    "    def _word_score(self, text: str, lang: str) -> float:\n",
    "        model, total, vocab = self.word_models[lang]\n",
    "        words = re.findall(r\"\\w+\", text.lower())\n",
    "        if len(words) < 2:\n",
    "            return -20.0\n",
    "\n",
    "        score = 0.0\n",
    "        for i in range(len(words) - 1):\n",
    "            bg = f\"{words[i]} {words[i+1]}\"\n",
    "            score += math.log((model.get(bg, 0) + self.alpha) /\n",
    "                              (total + self.alpha * vocab))\n",
    "        return score / (len(words) - 1)\n",
    "\n",
    "    def _char_score(self, text: str, lang: str) -> float:\n",
    "        model, total, vocab = self.char_models[lang]\n",
    "        words = re.findall(r\"\\w+\", text.lower())\n",
    "        if not words:\n",
    "            return -20.0\n",
    "\n",
    "        text = \"^\" + \"^\".join(words) + \"$\"\n",
    "        score = 0.0\n",
    "        for i in range(len(text) - 1):\n",
    "            bg = text[i:i+2]\n",
    "            score += math.log((model.get(bg, 0) + self.alpha) /\n",
    "                              (total + self.alpha * vocab))\n",
    "        return score / (len(text) - 1)\n",
    "\n",
    "    def extract(self, text: str) -> np.ndarray:\n",
    "        feats = []\n",
    "        for lang in self.languages:\n",
    "            feats.append(self._word_score(text, lang))\n",
    "        for lang in self.languages:\n",
    "            feats.append(self._char_score(text, lang))\n",
    "        return np.array(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWd2fCGJXheR"
   },
   "source": [
    "# Quantum Weight Optimization Using Variational Circuits\n",
    "\n",
    "This class uses a variational quantum circuit to learn optimal weights for classical language features.\n",
    "Each circuit layer applies parameterized rotation gates followed by entangling gates.\n",
    "Quantum measurement probabilities are mapped to feature weights and normalized.\n",
    "These weights are trained using a classical optimizer to minimize cross-entropy loss.\n",
    "The learned weights are later used for fast, classical language identification inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8D-bHnQVWn6t"
   },
   "outputs": [],
   "source": [
    "\n",
    "class QuantumWeightOptimizer:\n",
    "    def __init__(self, num_qubits=4, layers=4):\n",
    "        self.num_qubits = num_qubits\n",
    "        self.layers = layers\n",
    "        # Fix: Initialize ParameterVector with num_qubits * layers * 3 parameters\n",
    "        # as each qubit gets RX, RY, RZ per layer.\n",
    "        self.params = ParameterVector(\"θ\", num_qubits * layers * 3)\n",
    "        self.circuit = self._build_circuit()\n",
    "\n",
    "    def _build_circuit(self):\n",
    "        qc = QuantumCircuit(self.num_qubits)\n",
    "        p = 0\n",
    "        for _ in range(self.layers):\n",
    "            for q in range(self.num_qubits):\n",
    "                qc.rx(self.params[p], q); p += 1\n",
    "                qc.ry(self.params[p], q); p += 1\n",
    "                qc.rz(self.params[p], q); p += 1\n",
    "            for q in range(self.num_qubits - 1):\n",
    "                qc.cx(q, q + 1)\n",
    "        return qc\n",
    "\n",
    "    def get_weights(self, param_values):\n",
    "        state = Statevector.from_label(\"0\" * self.num_qubits)\n",
    "        state = state.evolve(\n",
    "            self.circuit.assign_parameters(dict(zip(self.params, param_values)))\n",
    "        )\n",
    "        probs = state.probabilities()\n",
    "        weights = probs[:6]\n",
    "        return weights / np.sum(weights)\n",
    "\n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        logger.info(\"Training quantum weight optimizer...\")\n",
    "\n",
    "        def loss_fn(params):\n",
    "            weights = self.get_weights(params)\n",
    "\n",
    "            # Corrected logic for logits calculation\n",
    "            weighted_feats = X * weights  # Element-wise multiplication for (num_samples, 6)\n",
    "            # Reshape (num_samples, 6) -> (num_samples, 2, 3) and sum over the 2nd axis to get (num_samples, 3)\n",
    "            logits = weighted_feats.reshape(X.shape[0], 2, 3).sum(axis=1)\n",
    "\n",
    "            exp = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "            probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "            return -np.mean(np.log(probs[np.arange(len(y)), y]))\n",
    "\n",
    "        init = np.random.uniform(-np.pi, np.pi, len(self.params))\n",
    "        result = optimize.minimize(\n",
    "            loss_fn, init, method=\"COBYLA\", options={\"maxiter\": 120}\n",
    "        )\n",
    "\n",
    "        self.best_params = result.x\n",
    "        self.weights = self.get_weights(self.best_params)\n",
    "        logger.info(f\"Quantum training complete. Weights: {self.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3eL3sheX5pL"
   },
   "source": [
    "# Hybrid Quantum–Classical Language Prediction\n",
    "\n",
    "This class performs fast language prediction using quantum-learned feature weights.\n",
    "Each input sentence is converted into word and character bigram features.\n",
    "The features are weighted using parameters learned by the quantum optimizer.\n",
    "Scores are aggregated per language to produce final predictions.\n",
    "This design enables real-time inference while preserving quantum optimization benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PYT2yDb1Xqwc"
   },
   "outputs": [],
   "source": [
    "class HybridLanguageIdentifier:\n",
    "    def __init__(self, extractor: FeatureExtractor, weights: np.ndarray):\n",
    "        self.extractor = extractor\n",
    "        self.weights = weights\n",
    "        self.languages = [\"twi\", \"eng\", \"fra\"]\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        feats = self.extractor.extract(text)\n",
    "        scores = feats * self.weights\n",
    "        scores = scores.reshape(2, 3).sum(axis=0)\n",
    "        return self.languages[np.argmax(scores)]\n",
    "\n",
    "    def predict_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.predict(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nn2jgOnwvtuR",
    "outputId": "ab3eda5d-6a15-412d-db0a-4dcda105eea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "RESULTS\n",
      "==============================\n",
      "Accuracy (CSV Dataset Test): 0.8298\n",
      "Accuracy (Generated Sentences): 0.7158\n",
      "==============================\n",
      "\n",
      "Detailed Report (Generated Sentences):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         eng       0.56      0.93      0.70     33175\n",
      "         fra       0.99      0.25      0.40     33378\n",
      "         twi       0.88      0.97      0.92     33447\n",
      "\n",
      "    accuracy                           0.72    100000\n",
      "   macro avg       0.81      0.72      0.67    100000\n",
      "weighted avg       0.81      0.72      0.67    100000\n",
      "\n",
      "\n",
      "Live Samples: [('Yɛreyɛ nhyehyɛeɛ sɛ yɛbɛkɔ mmepɔw so ɔsram a ɛdi hɔ yi.', 'twi'), ('The weather is nice', 'eng'), (\"C'est la vie\", 'fra')]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 2. Build Models\n",
    "    extractor = FeatureExtractor(alpha=0.1)\n",
    "    extractor.build_models(df)\n",
    "\n",
    "    # 3. Split Data: 80% Training, 20% Testing (on the actual dataset)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"lang_id\"], random_state=42)\n",
    "    X_train_raw = np.vstack([extractor.extract(t) for t in train_df[\"ngram\"]])\n",
    "    y_train_raw = train_df[\"lang_id\"].values - 1\n",
    "\n",
    "    # 4. Train Quantum Optimizer\n",
    "    quantum = QuantumWeightOptimizer(num_qubits=4, layers=4)\n",
    "    quantum.train(X_train_raw, y_train_raw)\n",
    "\n",
    "    # Initialize the Hybrid model\n",
    "    model = HybridLanguageIdentifier(extractor, quantum.weights)\n",
    "\n",
    "    # --- ACCURACY 1: TESTING ON THE DATASET (CSV) ---\n",
    "    logger.info(\"Evaluating on CSV Test Set...\")\n",
    "\n",
    "    # Removed 'random_state=42' as predict_batch does not accept it\n",
    "    dataset_preds = model.predict_batch(test_df[\"ngram\"].tolist())\n",
    "\n",
    "    # Convert lang_id (1,2,3) to names (\"twi\", \"eng\", \"fra\") for comparison\n",
    "    id_to_lang = {1: \"twi\", 2: \"eng\", 3: \"fra\"}\n",
    "    dataset_true = [id_to_lang[i] for i in test_df[\"lang_id\"]]\n",
    "\n",
    "    acc_dataset = accuracy_score(dataset_true, dataset_preds)\n",
    "\n",
    "    # --- ACCURACY 2: TESTING ON GENERATED SENTENCES ---\n",
    "    logger.info(\"Evaluating on Generated Sentences...\")\n",
    "\n",
    "    # Initialize generators for different locales\n",
    "    fake_en = Faker('en_US')\n",
    "    fake_fr = Faker('fr_FR')\n",
    "    # Faker doesn't support Twi well, so we use a larger word bank for it\n",
    "    twi_words = [\"medaase\", \"paa\", \"kyere\", \"obi\", \"nsuo\", \"akwaaba\", \"ɛyɛ\", \"onipa\"]\n",
    "\n",
    "    test_bigrams = []\n",
    "    gen_true_labels = []\n",
    "    target_langs = [\"twi\", \"eng\", \"fra\"]\n",
    "    for _ in range(100000):\n",
    "        lang_choice = np.random.choice(target_langs)\n",
    "\n",
    "        if lang_choice == \"eng\":\n",
    "            # Generate a random 4-word phrase\n",
    "            bigram = fake_en.sentence(nb_words=4).replace(\".\", \"\")\n",
    "        elif lang_choice == \"fra\":\n",
    "            bigram = fake_fr.sentence(nb_words=4).replace(\".\", \"\")\n",
    "        else:\n",
    "            bigram = \" \".join(np.random.choice(twi_words, 4))\n",
    "\n",
    "        test_bigrams.append(bigram)\n",
    "        gen_true_labels.append(lang_choice)\n",
    "\n",
    "    gen_preds = model.predict_batch(test_bigrams)\n",
    "    acc_generated = accuracy_score(gen_true_labels, gen_preds)\n",
    "\n",
    "    # --- FINAL OUTPUT ---\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"RESULTS\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Accuracy (CSV Dataset Test): {acc_dataset:.4f}\")\n",
    "    print(f\"Accuracy (Generated Sentences): {acc_generated:.4f}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    print(\"\\nDetailed Report (Generated Sentences):\")\n",
    "    print(classification_report(gen_true_labels, gen_preds, zero_division=0))\n",
    "\n",
    "    # Validation Sample\n",
    "    val_texts = [\"Yɛreyɛ nhyehyɛeɛ sɛ yɛbɛkɔ mmepɔw so ɔsram a ɛdi hɔ yi.\", \"The weather is nice\", \"C'est la vie\"]\n",
    "    val_preds = model.predict_batch(val_texts)\n",
    "    print(f\"\\nLive Samples: {list(zip(val_texts, val_preds))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeMW6WVffMMm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HL96xkpufMQL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIGEJIyz1gnN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
